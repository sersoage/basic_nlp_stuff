{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the use of the permutation test to assess the significance of coefficents learned in logistic regression (testing against the null that each $\\beta$ = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from random import choices\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            label=cols[0]\n",
    "            text=cols[1]\n",
    "            # assumes text is already tokenized\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "directory=\"../data/text_classification_sample_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY=read_data(\"%s/train.tsv\" % directory)\n",
    "devX, devY=read_data(\"%s/dev.tsv\" % directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(trainX, devX):\n",
    "    vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
    "\n",
    "    X_train = vectorizer.fit_transform(trainX)\n",
    "    X_dev = vectorizer.transform(devX)\n",
    "\n",
    "    return X_train, X_dev, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, trainY, le):\n",
    "    Y_train=le.transform(trainY)\n",
    "    logreg = linear_model.LogisticRegression(C=100, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    logreg.fit(X_train, Y_train)\n",
    "    return logreg\n",
    "    return logreg.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(logreg, devX_feat, devY, le):\n",
    "    Y_dev=le.transform(devY)\n",
    "    print(\"Accuracy: %.3f\" % logreg.score(devX_feat, Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_weights(coefs, label_encoder, vocab, p_values):\n",
    "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    sort_index = np.argsort(coefs)\n",
    "\n",
    "    print(label_encoder.inverse_transform([0])[0])\n",
    "    for k in sort_index[:25]:\n",
    "        print (\"%.5f\\t%s\\t%.4f\" % (coefs[k], reverse_vocab[k], p_values[k] ))\n",
    "\n",
    "    print(label_encoder.inverse_transform([1])[0])\n",
    "\n",
    "    for k in reversed(sort_index[-25:]):\n",
    "        print (\"%.5f\\t%s\\t%.4f\" % (coefs[k], reverse_vocab[k], p_values[k] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, vectorizer=featurize(trainX, devX)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(trainY)\n",
    "\n",
    "logreg=train(X_train, trainY, le)\n",
    "test(logreg, X_dev, devY, le)\n",
    "\n",
    "true_coefficients=logreg.coef_[0]\n",
    "\n",
    "# We'll set P=100 here to finish running in class, but set higher (e.g., 10000) for real applications\n",
    "P=100\n",
    "\n",
    "p_values=np.zeros(len(true_coefficients))\n",
    "permutedY=copy.deepcopy(trainY)\n",
    "\n",
    "for i in range(P):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    \n",
    "    # permute the values of Y so that they're now attached to random data points in X\n",
    "    shuffle(permutedY)\n",
    "    \n",
    "    # train logistic regression on that permuted dataset\n",
    "    permuted_logreg=train(X_train, permutedY, le)\n",
    "    coefficients=permuted_logreg.coef_[0]\n",
    "    \n",
    "    # test how often the coefficients learned from the permuted data are as extreme as\n",
    "    # the coefficients from the true data\n",
    "    for idx, coef in enumerate(coefficients):\n",
    "        if abs(true_coefficients[idx]) < abs(coef):\n",
    "            p_values[idx]+=1./P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_vocab = {v: k for k, v in vectorizer.vocabulary_.items()}            \n",
    "out=open(\"weights.txt\", \"w\")\n",
    "for idx, coef in enumerate(true_coefficients):\n",
    "    out.write(\"%.3f\\t%s\\t%.5f\\n\" % (coef, inverse_vocab[idx], p_values[idx]))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_weights(true_coefficients, le, vectorizer.vocabulary_, p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
