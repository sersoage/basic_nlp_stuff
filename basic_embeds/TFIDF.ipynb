{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores distribitional simliarity in a dataset of 10,000 Wikipedia articles (4.4M words), building high-dimensional, sparse representations for words from the distinct contexts they appear in.  These representations allow for analysis of the most similar words to a given query, and are interpretable with respect to the specific contexts that are most important for determining that two words are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import operator\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "window=2\n",
    "vocabSize=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"../data/wiki.10K.txt\"\n",
    "\n",
    "wiki_data=open(filename, encoding=\"utf-8\").read().lower().split(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll only create word representation for the most frequent K words\n",
    "\n",
    "def create_vocab(data):\n",
    "    word_representations={}\n",
    "    vocab=Counter()\n",
    "    for i, word in enumerate(data):\n",
    "        vocab[word]+=1\n",
    "\n",
    "    topK=[k for k,v in vocab.most_common(vocabSize)]\n",
    "    for k in topK:\n",
    "        word_representations[k]=defaultdict(float)\n",
    "    return word_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word representation for a word = its unigram distributional context (the unigrams that show\n",
    "# up in a window before and after its occurence)\n",
    "\n",
    "def count_unigram_context(data, word_representations):\n",
    "    for i, word in enumerate(data):\n",
    "        if word not in word_representations:\n",
    "            continue\n",
    "        start=i-window if i-window > 0 else 0\n",
    "        end=i+window+1 if i+window+1 < len(data) else len(data)\n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                word_representations[word][data[j]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_directional_context(data, word_representations):\n",
    "    for i, word in enumerate(data):\n",
    "        if word not in word_representations:\n",
    "            continue\n",
    "        start=i-window if i-window > 0 else 0\n",
    "        end=i+window+1 if i+window+1 < len(data) else len(data)\n",
    "        left=\"L: %s\" % ' '.join(data[start:i])\n",
    "        right=\"R: %s\" % ' '.join(data[i+1:end])\n",
    "        \n",
    "        word_representations[word][left]+=1\n",
    "        word_representations[word][right]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize a word represenatation vector that its L2 norm is 1.\n",
    "# we do this so that the cosine similarity reduces to a simple dot product\n",
    "\n",
    "def normalize(word_representations):\n",
    "    for word in word_representations:\n",
    "        total=0\n",
    "        for key in word_representations[word]:\n",
    "            total+=word_representations[word][key]*word_representations[word][key]\n",
    "            \n",
    "        total=math.sqrt(total)\n",
    "        for key in word_representations[word]:\n",
    "            word_representations[word][key]/=total\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_dot_product(dict1, dict2):\n",
    "    dot=0\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            dot+=dict1[key]*dict2[key]\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sim(word_representations, query):\n",
    "    if query not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query)\n",
    "        return None\n",
    "    \n",
    "    scores={}\n",
    "    for word in word_representations:\n",
    "        cosine=dictionary_dot_product(word_representations[query], word_representations[word])\n",
    "        scores[word]=cosine\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the K words with highest cosine similarity to a query in a set of word_representations\n",
    "def find_nearest_neighbors(word_representations, query, K):\n",
    "    scores=find_sim(word_representations, query)\n",
    "    if scores != None:\n",
    "        sorted_x = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        for idx, (k, v) in enumerate(sorted_x[:K]):\n",
    "            print(\"%s\\t%s\\t%.5f\" % (idx,k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the difference between `count_unigram_context` and `count_directional_context` for determining what counts as \"context\".  `count_unigram_context` counts an individual unigram in the bag of words around a target as a \"context\" variable, while `count_directional_context` counts the sequence of words before and after the word as a single \"context\"--and specifies the direction it occurs (to the left or right of the word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_representations=create_vocab(wiki_data)\n",
    "count_directional_context(wiki_data, word_representations)\n",
    "normalize(word_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tactor\t1.00000\n",
      "1\tpolitician\t0.54099\n",
      "2\tactress\t0.52242\n",
      "3\tcricketer\t0.42361\n",
      "4\tartist\t0.40005\n",
      "5\twriter\t0.38234\n",
      "6\tcyclist\t0.36833\n",
      "7\tmusician\t0.33385\n",
      "8\tdiplomat\t0.32010\n",
      "9\tpoet\t0.31124\n"
     ]
    }
   ],
   "source": [
    "find_nearest_neighbors(word_representations, \"actor\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the contexts shared between two words that have the most contribution\n",
    "# to the cosine similarity\n",
    "\n",
    "def find_shared_contexts(word_representations, query1, query2, K):\n",
    "    if query1 not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query1)\n",
    "        return None\n",
    "    \n",
    "    if query2 not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query2)\n",
    "        return None\n",
    "    \n",
    "    context_scores={}\n",
    "    dict1=word_representations[query1]\n",
    "    dict2=word_representations[query2]\n",
    "    \n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            score=dict1[key]*dict2[key]\n",
    "            context_scores[key]=score\n",
    "\n",
    "    sorted_x = sorted(context_scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    for idx, (k, v) in enumerate(sorted_x[:K]):\n",
    "        print(\"%s\\t%s\\t%.5f\" % (idx,k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tR: . he\t0.21961\n",
      "1\tL: an american\t0.13391\n",
      "2\tR: ) .\t0.11417\n",
      "3\tR: in the\t0.01410\n",
      "4\tL: an indian\t0.00761\n",
      "5\tL: a canadian\t0.00677\n",
      "6\tL: an english\t0.00564\n",
      "7\tR: of the\t0.00564\n",
      "8\tL: a french\t0.00423\n",
      "9\tR: , who\t0.00338\n"
     ]
    }
   ],
   "source": [
    "find_shared_contexts(word_representations, \"actor\", \"politician\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Fill out a function `scale_tfidf` below.  This function takes as input a dict of word_representations and scales the value for each context in `word_representations[word]` by its tf-idf score.  Use the term frequency for tf and ${N \\over |\\{d \\in D : t \\in d\\}|}$ for idf.  Here, tf measure the count of a *context* term for a particular *word*, and idf measures the number of distinct *words* a particular *context* is seen with.  This function should modify `word_representations` in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_tfidf(word_representations):\n",
    "    # count IDFs\n",
    "    idfs=defaultdict(int)\n",
    "    for term in word_representations:\n",
    "        for context in word_representations[term]:\n",
    "            idfs[context]+=1\n",
    "            \n",
    "    N=len(word_representations)\n",
    "    for word in word_representations:\n",
    "        for key in word_representations[word]:\n",
    "            word_representations[word][key]*=math.log(N/idfs[key])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_word_representations=create_vocab(wiki_data)\n",
    "count_directional_context(wiki_data, tf_idf_word_representations)\n",
    "scale_tfidf(tf_idf_word_representations)\n",
    "normalize(tf_idf_word_representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Compare the results the results of tf-idf scaling with the non-scaled results above.  How does scaling change the quality of the nearest neighbors, or the sensibility of the significant contexts?  Provide examples to support your claims using `find_nearest_neighbors` and `find_shared_contexts` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\ttwo\t1.00000\n",
      "1\tthree\t0.68766\n",
      "2\tfour\t0.61315\n",
      "3\tterm\t0.54894\n",
      "4\tsong\t0.54867\n",
      "5\tchurch\t0.54464\n",
      "6\tmain\t0.54448\n",
      "7\tcompany\t0.54311\n",
      "8\tband\t0.54142\n",
      "9\tbuilding\t0.54053\n",
      "\n",
      "0\ttwo\t1.00000\n",
      "1\tthree\t0.42725\n",
      "2\tfour\t0.33787\n",
      "3\tfive\t0.31393\n",
      "4\tseveral\t0.23219\n",
      "5\tsix\t0.22343\n",
      "6\tfew\t0.22313\n",
      "7\teight\t0.22192\n",
      "8\tseven\t0.20937\n",
      "9\ttwenty\t0.18513\n"
     ]
    }
   ],
   "source": [
    "query=\"two\"\n",
    "find_nearest_neighbors(word_representations, query, 10)\n",
    "print()\n",
    "find_nearest_neighbors(tf_idf_word_representations, query, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tL: of the\t0.15490\n",
      "1\tL: . the\t0.09570\n",
      "2\tR: of the\t0.04114\n",
      "3\tL: the first\t0.03932\n",
      "4\tL: , and\t0.03102\n",
      "5\tL: there are\t0.03037\n",
      "6\tR: years later\t0.02998\n",
      "7\tL: , the\t0.02032\n",
      "8\tL: one of\t0.01649\n",
      "9\tR: years ,\t0.01081\n",
      "\n",
      "0\tR: years later\t0.05778\n",
      "1\tL: there are\t0.02744\n",
      "2\tR: days later\t0.01653\n",
      "3\tL: one of\t0.01490\n",
      "4\tR: years ,\t0.01324\n",
      "5\tR: years .\t0.01194\n",
      "6\tL: the first\t0.01144\n",
      "7\tR: or more\t0.01066\n",
      "8\tL: the next\t0.01057\n",
      "9\tL: divided into\t0.01024\n"
     ]
    }
   ],
   "source": [
    "query1=\"two\"\n",
    "query2=\"three\"\n",
    "find_shared_contexts(word_representations, query1, query2, 10)\n",
    "print()\n",
    "find_shared_contexts(tf_idf_word_representations, query1, query2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
