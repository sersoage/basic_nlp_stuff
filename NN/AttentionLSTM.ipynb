{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the use of a bidirectional LSTM with attention for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import Dense, Input, Embedding, Lambda, Layer, Multiply, Dropout, Dot, Bidirectional, LSTM\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from math import sqrt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename, max_vocab_size):\n",
    "\n",
    "    vocab={}\n",
    "    embeddings=[]\n",
    "    with open(filename) as file:\n",
    "        \n",
    "        cols=file.readline().split(\" \")\n",
    "        num_words=int(cols[0])\n",
    "        size=int(cols[1])\n",
    "        embeddings.append(np.zeros(size))  # 0 = 0 padding if needed\n",
    "        embeddings.append(np.zeros(size))  # 1 = UNK\n",
    "        vocab[\"_0_\"]=0\n",
    "        vocab[\"_UNK_\"]=1\n",
    "        \n",
    "        for idx,line in enumerate(file):\n",
    "\n",
    "            if idx+2 >= max_vocab_size:\n",
    "                break\n",
    "\n",
    "            cols=line.rstrip().split(\" \")\n",
    "            val=np.array(cols[1:])\n",
    "            word=cols[0]\n",
    "            \n",
    "            embeddings.append(val)\n",
    "            vocab[word]=idx+2\n",
    "\n",
    "    return np.array(embeddings), vocab, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, vocab):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            label=cols[0]\n",
    "            # assumes text is already tokenized\n",
    "            text=cols[1].split(\" \")\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ids(docs, vocab, max_length=200):\n",
    "    \n",
    "    doc_ids=[]\n",
    "    \n",
    "    for doc in docs:\n",
    "        wids=[]\n",
    "        for token in doc[:max_length]:\n",
    "            val = vocab[token.lower()] if token.lower() in vocab else 1\n",
    "            wids.append(val)\n",
    "        \n",
    "        # pad each document to constant width\n",
    "        for i in range(len(wids),max_length):\n",
    "            wids.append(0)\n",
    "\n",
    "        doc_ids.append(wids)\n",
    "\n",
    "    return np.array(doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't downloaded the glove vectors, do so first -- the top 50K words in the \"Common Crawl (42B)\"  vectors (300-dimensional) can be found here: [glove.42B.300d.50K.txt](https://drive.google.com/file/d/1n1jt0UIdI3CD26cY1EIeks39XH5S8O8M/view?usp=sharing); download it and place  in your `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file=\"../data/glove.42B.300d.50K.txt\"\n",
    "glove_in_w2v_format=\"../data/glove.42B.300d.50K.w2v.txt\"\n",
    "_ = glove2word2vec(glove_file, glove_in_w2v_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, vocab, embedding_size=load_embeddings(\"../data/glove.42B.300d.50K.w2v.txt\", 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "directory=\"../data/lmrd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainText, trainY=read_data(\"%s/train.tsv\" % directory, vocab)\n",
    "devText, devY=read_data(\"%s/dev.tsv\" % directory, vocab)\n",
    "testText, testY=read_data(\"%s/test.tsv\" % directory, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = get_word_ids(trainText, vocab, max_length=200)\n",
    "devX = get_word_ids(devText, vocab, max_length=200)\n",
    "testX = get_word_ids(testText, vocab, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(trainY)\n",
    "Y_train=np.array(le.transform(trainY))\n",
    "Y_dev=np.array(le.transform(devY))\n",
    "Y_test=np.array(le.transform(testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayerMasking(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(AttentionLayerMasking, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_embedding_dim=input_shape[-1]\n",
    "        \n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                            shape=(input_embedding_dim,1),\n",
    "                            initializer='uniform',\n",
    "                            trainable=True)\n",
    "        super(AttentionLayerMasking, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        \n",
    "        # dot product \n",
    "        x=K.dot(x, self.kernel)\n",
    "        # exponentiate\n",
    "        x=K.exp(x)\n",
    "        \n",
    "        # zero out elements that are masked\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.expand_dims(mask, axis=-1)\n",
    "            x = x * mask\n",
    "        \n",
    "        # normalize by sum\n",
    "        x /= K.sum(x, axis=1, keepdims=True)\n",
    "        x=K.squeeze(x, axis=2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Implement a BiLSTM with attention. Feel free to base your code on the models in Attention.ipynb and LSTM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_with_attention_masking(embeddings, lstm_size=25, dropout_rate=0.25):\n",
    "\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "    \n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    word_embedding_layer = Embedding(vocab_size,\n",
    "                                    word_embedding_dim,\n",
    "                                    weights=[embeddings], \n",
    "                                    mask_zero=True,\n",
    "                                    trainable=False)\n",
    "\n",
    "    \n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    bilstm_output = Bidirectional(LSTM(lstm_size, return_sequences=True, activation='tanh', dropout=dropout_rate), merge_mode='concat')(embedded_sequences)\n",
    "\n",
    "    # first let's transform each word embedding into a new vector to use for measuring its importance\n",
    "    attention_key_dim=300\n",
    "    attention_input=Dense(attention_key_dim, activation='tanh')(bilstm_output)\n",
    "\n",
    "    # next we'll pass those transformed inputs through an attention layer, getting back a normalized\n",
    "    # attention value a_i for each token i; \\forall i, 0 <= a_i <= 1; for a document with N words, \n",
    "    # \\sum_{i=0}^N a_i = 1\n",
    "    \n",
    "    attention_output = AttentionLayerMasking(word_embedding_dim, name=\"attention\")(attention_input)\n",
    "    \n",
    "    # now let's multiply those attention weights by original inputs to get a weighted average over them\n",
    "    document_representation = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=1), name='dot')([attention_output,bilstm_output])\n",
    "\n",
    "    x=Dense(1, activation=\"sigmoid\")(document_representation)\n",
    "\n",
    "    model = Model(inputs=word_sequence_input, outputs=x)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_attention_model=get_bilstm_with_attention_masking(embeddings, lstm_size=25, dropout_rate=0.25)\n",
    "print (bilstm_attention_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=bilstm_attention_model\n",
    "\n",
    "modelName=\"bilstm_attention_model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(modelName, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "model.fit(trainX, Y_train, \n",
    "            validation_data=(devX, Y_dev),\n",
    "            epochs=30, batch_size=128,\n",
    "            callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the accuracy of your model on the test data?  Report the accuracy score with 95% confidence intervals.  Feel free to use the dev data for model selection (e.g., to hyperparameter choices like the size of hidden LSTM state, etc.), but be careful not to use the test data for this.  See keras [model.predict](https://keras.io/models/model/#predict) to generate predictions for a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial_confidence_intervals(predictions, truth, confidence_level=0.95):\n",
    "    correct=[]\n",
    "    for pred, gold in zip(predictions, truth):\n",
    "        correct.append(int(pred==gold))\n",
    "        \n",
    "    success_rate=np.mean(correct)\n",
    "\n",
    "    # two-tailed test\n",
    "    critical_value=(1-confidence_level)/2\n",
    "    # ppf finds z such that p(X < z) = critical_value\n",
    "    z_alpha=-1*norm.ppf(critical_value)\n",
    "    \n",
    "    # the standard error is the square root of the variance/sample size\n",
    "    # the variance for a binomial test is p*(1-p)\n",
    "    standard_error=sqrt((success_rate*(1-success_rate))/len(correct))\n",
    "\n",
    "    lower=success_rate-z_alpha*standard_error\n",
    "    upper=success_rate+z_alpha*standard_error\n",
    "    print(\"%.3f, %s%% Confidence interval: [%.3f,%.3f]\" % (success_rate, confidence_level*100, lower, upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=bilstm_attention_model\n",
    "\n",
    "model.load_weights(\"bilstm_attention_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(testX, batch_size=128)\n",
    "binarized_predictions=predictions > .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_confidence_interval(binarized_predictions, Y_test, confidence_level=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Take the sentence \"I do not like this movie.\" How is representing this sentence by using attention over the individual word embeddings different from representing it with attention over the output of each time step in an bidirectional LSTM?  What information does the LSTM output encode that individual word embeddings don't have access to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A3: Word embeddings encode information about the word *type* but not about its specific use in context; the output of an LSTM at time t encodes information about the context a word *token* was used in -- for a single forward LSTM, the context of the sequence from word 1 through word t; for a BiLSTM, the context of the entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
