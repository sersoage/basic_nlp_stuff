{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the use of attention for text classification, comparing a model that represents a document by averaging its word embeddings to one that uses an attention mechanism to compute a weighted average over those embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import Dense, Input, Embedding, Lambda, Layer, Multiply, Dropout, Dot\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename, max_vocab_size):\n",
    "\n",
    "    vocab={}\n",
    "    embeddings=[]\n",
    "    with open(filename) as file:\n",
    "        \n",
    "        cols=file.readline().split(\" \")\n",
    "        num_words=int(cols[0])\n",
    "        size=int(cols[1])\n",
    "        embeddings.append(np.zeros(size))  # 0 = 0 padding if needed\n",
    "        embeddings.append(np.zeros(size))  # 1 = UNK\n",
    "        vocab[\"_0_\"]=0\n",
    "        vocab[\"_UNK_\"]=1\n",
    "        \n",
    "        for idx,line in enumerate(file):\n",
    "\n",
    "            if idx+2 >= max_vocab_size:\n",
    "                break\n",
    "\n",
    "            cols=line.rstrip().split(\" \")\n",
    "            val=np.array(cols[1:])\n",
    "            word=cols[0]\n",
    "            \n",
    "            embeddings.append(val)\n",
    "            vocab[word]=idx+2\n",
    "\n",
    "    return np.array(embeddings), vocab, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, vocab):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            label=cols[0]\n",
    "            # assumes text is already tokenized\n",
    "            text=cols[1].split(\" \")\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ids(docs, vocab, max_length=200):\n",
    "    \n",
    "    doc_ids=[]\n",
    "    \n",
    "    for doc in docs:\n",
    "        wids=[]\n",
    "        for token in doc[:max_length]:\n",
    "            val = vocab[token.lower()] if token.lower() in vocab else 1\n",
    "            wids.append(val)\n",
    "        \n",
    "        # pad each document to constant width\n",
    "        for i in range(len(wids),max_length):\n",
    "            wids.append(0)\n",
    "\n",
    "        doc_ids.append(wids)\n",
    "\n",
    "    return np.array(doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't downloaded the glove vectors, do so first -- the top 50K words in the \"Common Crawl (42B)\"  vectors (300-dimensional) can be found here: [glove.42B.300d.50K.txt](https://drive.google.com/file/d/1n1jt0UIdI3CD26cY1EIeks39XH5S8O8M/view?usp=sharing); download it and place  in your `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file=\"../data/glove.42B.300d.50K.txt\"\n",
    "glove_in_w2v_format=\"../data/glove.42B.300d.50K.w2v.txt\"\n",
    "_ = glove2word2vec(glove_file, glove_in_w2v_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, vocab, embedding_size=load_embeddings(\"../data/glove.42B.300d.50K.w2v.txt\", 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "directory=\"../data/lmrd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainText, trainY=read_data(\"%s/train.tsv\" % directory, vocab)\n",
    "devText, devY=read_data(\"%s/dev.tsv\" % directory, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = get_word_ids(trainText, vocab, max_length=200)\n",
    "devX = get_word_ids(devText, vocab, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(trainY)\n",
    "Y_train=np.array(le.transform(trainY))\n",
    "Y_dev=np.array(le.transform(devY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try a simple model that represents a document by averaging the embeddings for the words it contains.  We'll again use appropriate masking to accommodate zero-padded sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAveragePooling1D(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        super(MaskedAveragePooling1D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.repeat(mask, x.shape[-1])\n",
    "            mask = tf.transpose(mask, [0,2,1])\n",
    "            # zero out the elements of x that are masked\n",
    "            x = x * mask\n",
    "            \n",
    "        # sum the modified input, but normalize only over the number of non-masked time steps\n",
    "        return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_average(embeddings):\n",
    "\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "    \n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    word_embedding_layer = Embedding(vocab_size,\n",
    "                                    word_embedding_dim,\n",
    "                                    weights=[embeddings],\n",
    "                                    mask_zero=True,\n",
    "                                    trainable=False)\n",
    "\n",
    "    \n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    x=MaskedAveragePooling1D()(embedded_sequences)\n",
    "    \n",
    "    predictions=Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=word_sequence_input, outputs=predictions)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model=get_embedding_average(embeddings)\n",
    "print (embedding_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=embedding_model\n",
    "\n",
    "modelName=\"embedding_model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(modelName, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "model.fit(trainX, Y_train, \n",
    "            validation_data=(devX, Y_dev),\n",
    "            epochs=30, batch_size=128,\n",
    "            callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's add attention to that simple model to learn a *weighted* average over words---giving more weight to words in the document that are more important for representing the document for the purpose of this classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayerMasking(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(AttentionLayerMasking, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_embedding_dim=input_shape[-1]\n",
    "        \n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                            shape=(input_embedding_dim,1),\n",
    "                            initializer='uniform',\n",
    "                            trainable=True)\n",
    "        super(AttentionLayerMasking, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        \n",
    "        # dot product \n",
    "        x=K.dot(x, self.kernel)\n",
    "        # exponentiate\n",
    "        x=K.exp(x)\n",
    "        \n",
    "        # zero out elements that are masked\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.expand_dims(mask, axis=-1)\n",
    "            x = x * mask\n",
    "        \n",
    "        # normalize by sum\n",
    "        x /= K.sum(x, axis=1, keepdims=True)\n",
    "        x=K.squeeze(x, axis=2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_with_attention_masking(embeddings):\n",
    "\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "    \n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    word_embedding_layer = Embedding(vocab_size,\n",
    "                                    word_embedding_dim,\n",
    "                                    weights=[embeddings], \n",
    "                                    mask_zero=True,\n",
    "                                    trainable=False)\n",
    "\n",
    "    \n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    \n",
    "    # first let's transform each word embedding into a new vector to use for measuring its importance\n",
    "    attention_key_dim=300\n",
    "    attention_input=Dense(attention_key_dim, activation='tanh')(embedded_sequences)\n",
    "\n",
    "    # next we'll pass those transformed inputs through an attention layer, getting back a normalized\n",
    "    # attention value a_i for each token i; \\forall i, 0 <= a_i <= 1; for a document with N words, \n",
    "    # \\sum_{i=0}^N a_i = 1\n",
    "    \n",
    "    attention_output = AttentionLayerMasking(word_embedding_dim, name=\"attention\")(attention_input)\n",
    "    \n",
    "    # now let's multiply those attention weights by original inputs to get a weighted average over them\n",
    "    document_representation = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=1), name='dot')([attention_output,embedded_sequences])\n",
    "\n",
    "    x=Dense(1, activation=\"sigmoid\")(document_representation)\n",
    "\n",
    "    model = Model(inputs=word_sequence_input, outputs=x)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_attention_model=get_embedding_with_attention_masking(embeddings)\n",
    "print (embedding_attention_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=embedding_attention_model\n",
    "\n",
    "modelName=\"embedding_attention_model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(modelName, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "model.fit(trainX, Y_train, \n",
    "            validation_data=(devX, Y_dev),\n",
    "            epochs=30, batch_size=128,\n",
    "            callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore what words in a document a learned attention model is attending to.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best saved model\n",
    "\n",
    "model=embedding_attention_model\n",
    "model.load_weights(\"embedding_attention_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(model, doc):\n",
    "    \n",
    "    words=doc.split(\" \")\n",
    "    text = get_word_ids([words], vocab, max_length=len(words))\n",
    "   \n",
    "    inp = model.input                                    \n",
    "    outputs = [layer.output for layer in model.layers[1:]]       \n",
    "    functor = K.function([inp, K.learning_phase()], outputs) \n",
    "\n",
    "    test = text[0]\n",
    "    orig=words\n",
    "    attention_weights=[]\n",
    "    test=test.reshape((1,len(words)))\n",
    "    layer_outs = functor([test, 0.])\n",
    "\n",
    "    # in this model, attention is the third layer\n",
    "    attention_layer=layer_outs[2]\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        val=attention_layer[0,i]\n",
    "        attention_weights.append(val)\n",
    "        print (\"%.3f\\t%s\" % (val, orig[i]))\n",
    "        \n",
    "    df = pd.DataFrame({'words':orig, 'attention':attention_weights})\n",
    "    ax = df.plot.bar(x='words', y='attention', figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"i love this movie !\"\n",
    "analyze(model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"i do not love this movie !\"\n",
    "analyze(model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
