{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores LSTMs for text classification, representing a document by:\n",
    "\n",
    "* the final state of a LSTM\n",
    "* the final states of a Bidirectional LSTM\n",
    "* averaging the outputs of each time step in a BiLSTM\n",
    "* maxing the outputs of each time step in a BiLSTM\n",
    "\n",
    "This notebook also focuses on appropriate masking for averaging/max-pooling in padded sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import Dense, Input, Embedding, GlobalAveragePooling1D, Lambda, Layer, Multiply, GlobalMaxPooling1D, Conv1D, Concatenate, Dropout, LSTM, Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename, max_vocab_size):\n",
    "\n",
    "    vocab={}\n",
    "    embeddings=[]\n",
    "    with open(filename) as file:\n",
    "        \n",
    "        cols=file.readline().split(\" \")\n",
    "        num_words=int(cols[0])\n",
    "        size=int(cols[1])\n",
    "        embeddings.append(np.zeros(size))  # 0 = 0 padding if needed\n",
    "        embeddings.append(np.zeros(size))  # 1 = UNK\n",
    "        vocab[\"_0_\"]=0\n",
    "        vocab[\"_UNK_\"]=1\n",
    "        \n",
    "        for idx,line in enumerate(file):\n",
    "\n",
    "            if idx+2 >= max_vocab_size:\n",
    "                break\n",
    "\n",
    "            cols=line.rstrip().split(\" \")\n",
    "            val=np.array(cols[1:])\n",
    "            word=cols[0]\n",
    "            \n",
    "            embeddings.append(val)\n",
    "            vocab[word]=idx+2\n",
    "\n",
    "    return np.array(embeddings), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, vocab):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            label=cols[0]\n",
    "            # assumes text is already tokenized\n",
    "            text=cols[1].split(\" \")\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ids(docs, vocab, max_length=200):\n",
    "    \n",
    "    doc_ids=[]\n",
    "    \n",
    "    for doc in docs:\n",
    "        wids=[]\n",
    "\n",
    "        for token in doc[:max_length]:\n",
    "            val = vocab[token.lower()] if token.lower() in vocab else 1\n",
    "            wids.append(val)\n",
    "        \n",
    "        # pad each document to constant width\n",
    "        for i in range(len(wids),max_length):\n",
    "            wids.append(0)\n",
    "\n",
    "        doc_ids.append(wids)\n",
    "\n",
    "    return np.array(doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, vocab=load_embeddings(\"../data/glove.42B.300d.50K.w2v.txt\", 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "directory=\"../data/text_classification_sample_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainText, trainY=read_data(\"%s/train.tsv\" % directory, vocab)\n",
    "devText, devY=read_data(\"%s/dev.tsv\" % directory, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = get_word_ids(trainText, vocab, max_length=200)\n",
    "devX = get_word_ids(devText, vocab, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(trainY)\n",
    "Y_train=np.array(le.transform(trainY))\n",
    "Y_dev=np.array(le.transform(devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    print (model.summary())\n",
    "    model.fit(trainX, Y_train, \n",
    "                validation_data=(devX, Y_dev),\n",
    "                epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll train a simple LSTM and represent the document by the summary vector output by the final state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_lstm(embeddings, lstm_size=25, dropout_rate=0.2):\n",
    "\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "    \n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    word_embedding_layer = Embedding(vocab_size,\n",
    "                                    word_embedding_dim,\n",
    "                                    weights=[embeddings],\n",
    "                                    mask_zero=True,\n",
    "                                    trainable=False)\n",
    "\n",
    "    \n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    \n",
    "    lstm = LSTM(lstm_size, return_sequences=False, activation='tanh', dropout=dropout_rate)(embedded_sequences)\n",
    "  \n",
    "    predictions=Dense(1, activation=\"sigmoid\")(lstm)\n",
    "\n",
    "    model = Model(inputs=word_sequence_input, outputs=predictions)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(get_simple_lstm(embeddings, lstm_size=25, dropout_rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll represent a document by two concatenated vectors: the output of the final state of a forward LSTM and the output of the final state of the backward LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_bilstm(embeddings, lstm_size=25, dropout_rate=0.2):\n",
    "\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "\n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    word_embedding_layer = Embedding(vocab_size,\n",
    "                                    word_embedding_dim,\n",
    "                                    weights=[embeddings],\n",
    "                                    mask_zero=True,\n",
    "                                    trainable=False)\n",
    "\n",
    "    \n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    \n",
    "    bi_lstm = Bidirectional(LSTM(lstm_size, return_sequences=False, activation='tanh', dropout=dropout_rate), merge_mode='concat')(embedded_sequences)\n",
    "  \n",
    "    predictions=Dense(1, activation=\"sigmoid\")(bi_lstm)\n",
    "\n",
    "    model = Model(inputs=word_sequence_input, outputs=predictions)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(get_simple_bilstm(embeddings, lstm_size=25, dropout_rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final state is often a poor representation of the entire sequence, since it can lose information from the beginning of the sequence.  Let's define a few other layers that can aggregate information across the *entire* sequence, using the information that's output from the LSTM at each time step.  We need to define these custom layers in keras to accomodate zero-padding appropriately (see [here](https://stackoverflow.com/questions/39510809/mean-or-max-pooling-with-masking-support-in-keras/39534110#39534110) for discussion, where these functions originate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAveragePooling1D(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        super(MaskedAveragePooling1D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.repeat(mask, x.shape[-1])\n",
    "            mask = tf.transpose(mask, [0,2,1])\n",
    "            # zero out the elements of x that are masked\n",
    "            x = x * mask\n",
    "            \n",
    "        # sum the modified input, but normalize only over the number of non-masked time steps\n",
    "        return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMaxPooling1D(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        super(MaskedMaxPooling1D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if mask is not None:\n",
    "            # take the logical negation of the mask (all 1s become 0 and 0s become 1s)\n",
    "            mask=tf.logical_not(mask)\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.repeat(mask, x.shape[-1])    \n",
    "            mask = tf.transpose(mask, [0,2,1])\n",
    "            \n",
    "            # subtract a big number from each masked input (so that it won't be the max)\n",
    "            mask *= 10000\n",
    "            x = x - mask\n",
    "        \n",
    "        # max over the modified input along the temporal dimension\n",
    "        return K.max(x, axis=1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore the use of these functions to aggregate information over the whole sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_with_average_pooling(embeddings, lstm_size=25, dropout_rate=0.2):\n",
    "\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "\n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    word_embedding_layer = Embedding(vocab_size,\n",
    "                                    word_embedding_dim,\n",
    "                                    weights=[embeddings], \n",
    "                                    mask_zero=True,\n",
    "                                    trainable=False)\n",
    "\n",
    "    \n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    \n",
    "    x = Bidirectional(LSTM(lstm_size, return_sequences=True, activation='tanh', dropout=dropout_rate), merge_mode='concat')(embedded_sequences)\n",
    "    x=MaskedAveragePooling1D()(x)\n",
    "\n",
    "    x=Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=word_sequence_input, outputs=x)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(get_bilstm_with_average_pooling(embeddings, lstm_size=25, dropout_rate=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_with_max_pooling(embeddings, lstm_size=25, dropout_rate=0.2):\n",
    "\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "\n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    word_embedding_layer = Embedding(vocab_size,\n",
    "                                    word_embedding_dim,\n",
    "                                    weights=[embeddings], \n",
    "                                     mask_zero=True,\n",
    "                                    trainable=False)\n",
    "\n",
    "    \n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    \n",
    "    x = Bidirectional(LSTM(lstm_size, return_sequences=True, activation='tanh', dropout=dropout_rate), merge_mode='concat')(embedded_sequences)\n",
    "    x=MaskedMaxPooling1D()(x)\n",
    "\n",
    "    x=Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=word_sequence_input, outputs=x)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(get_bilstm_with_max_pooling(embeddings, lstm_size=25, dropout_rate=0.2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
