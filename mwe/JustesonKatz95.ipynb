{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores identifying multiword expressions using the part-of-speech filtering technique of Justeson and Katz (1995), \"[Technical terminology: some linguistic properties and an algorithm for identification in text](https://brenocon.com/JustesonKatz1995.pdf)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['ner,parser'])\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser')\n",
    "\n",
    "# workaround for those getting an error loading the sapcy 'en' module:\n",
    "# nlp = spacy.load('en_core_web_sm', disable=['ner,parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(filename):\n",
    "    \n",
    "    \"\"\" Read the first 1000 lines of an input file \"\"\"\n",
    "    tokens=[]\n",
    "    with open(filename) as file:\n",
    "        for idx,line in enumerate(file):\n",
    "            tokens.extend(nlp(line))\n",
    "            if idx > 1000:\n",
    "                break\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=getTokens(\"../data/wiki.10K.txt\")\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[x.text for x in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simplify the POS tags to make the regex easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives=set([\"JJ\", \"JJR\", \"JJS\"])\n",
    "nouns=set([\"NN\", \"NNS\", \"NNP\", \"NNPS\"])\n",
    "\n",
    "taglist=[]\n",
    "for x in tokens:\n",
    "    if x.tag_ in adjectives:\n",
    "        taglist.append(\"ADJ\")\n",
    "    elif x.tag_ in nouns:\n",
    "        taglist.append(\"NOUN\")\n",
    "    elif x.tag == \"IN\":\n",
    "        taglist.append(\"PREP\")\n",
    "    else:\n",
    "        taglist.append(\"O\")\n",
    "                \n",
    "tags=' '.join(taglist)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChar2TokenMap(tags):\n",
    "    \n",
    "    \"\"\"  We'll search over the postag sequence, so we need to get the token ID for any\n",
    "    character to be able to match the word token. \"\"\"\n",
    "    \n",
    "    ws=re.compile(\" \")\n",
    "    char2token={}\n",
    "\n",
    "    lastStart=0\n",
    "    for idx, m in enumerate(ws.finditer(tags)):\n",
    "        char2token[lastStart]=idx\n",
    "        lastStart=m.start()+1\n",
    "        \n",
    "    return char2token\n",
    "\n",
    "def getToken(tokenId, char2token):\n",
    "    \n",
    "    \"\"\" Find the token ID for given character in the POS sequence \"\"\"\n",
    "    while(tokenId > 0):\n",
    "        if tokenId in char2token:\n",
    "            return char2token[tokenId]\n",
    "        tokenId-=1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2token=getChar2TokenMap(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find all sequences of POS tags that match the Justeson and Katz pattern of `(((ADJ|NOUN) )+|((ADJ|NOUN) )*(NOUN PREP )((ADJ|NOUN) )*)NOUN`\n",
    "\n",
    "\"In words, a candidate term is a multi-word noun phrase; and it either is a string of nouns and/or adjectives, ending in a noun, or it consists of two such strings, separated by a single preposition.\" (JK 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(\"(((ADJ|NOUN) )+|((ADJ|NOUN) )*(NOUN PREP )((ADJ|NOUN) )*)NOUN\")\n",
    "\n",
    "mweCount=Counter()\n",
    "\n",
    "for m in p.finditer(tags):\n",
    "    startToken=getToken(m.start(),char2token)\n",
    "    endToken=getToken(m.end(),char2token)\n",
    "    mwe=' '.join(words[startToken:endToken+1])\n",
    "    mweCount[mwe]+=1\n",
    "\n",
    "for k,v in mweCount.most_common(100):\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define our MWE dictionary to be the 1000 most frequent sequences matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mwe=[k for (k,v) in mweCount.most_common(1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's transform each MWE into a single token (e.g., replace `New York City` with `New_York_City`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceMWE(text, mweList):\n",
    "    \n",
    "    \"\"\" Replace all instances of MWEs in text with single token \n",
    "    \n",
    "    MWEs are ranked from longest to shortest so that longest replacements are made first (e.g.,\n",
    "    \"New York City\" is matched first before \"New York\")\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sorted_by_length = sorted(mweList, key=len, reverse=True)\n",
    "    for mwe in sorted_by_length:\n",
    "        text=re.sub(re.escape(mwe), re.sub(\" \", \"_\", mwe), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedText=replaceMWE(\"The New York Times is located in New York City\", my_mwe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processedText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Plug in your own data into `getTokens` above and identify the MWE it contains.  How do you think MWE would perform for your classification task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
