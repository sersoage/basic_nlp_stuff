{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores part-of-speech tagging through its impact on keyword extraction. Keyphrase extraction is a task designed to select a small number of terms (or phrases) from a document that best represent its content.  Here we'll use a tf-idf metric for ranking terms in a document, and use POS information to filter those terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, glob, os, operator, math, random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['ner,parser'])\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you get a word and its POS tag from SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_tags(text):\n",
    "    \"\"\" Get spacy tags for an input text \"\"\"\n",
    "    doc=nlp(text)\n",
    "    for word in doc:\n",
    "        print(word.text, word.tag_)\n",
    "\n",
    "get_spacy_tags(\"Time flies like an arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(inputDir):\n",
    "    \"\"\" Read in movie documents (all ending in .txt) from an input folder\"\"\"\n",
    "    \n",
    "    docs=[]\n",
    "    for filename in glob.glob(os.path.join(inputDir, '*.txt')):\n",
    "        with open(filename) as file:\n",
    "            docs.append((filename, nlp(file.read())))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory with 2000 movie summaries from Wikipedia\n",
    "inputDir=\"../data/movie_summaries/\"\n",
    "original_docs=read_docs(inputDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. We covered tf-idf in lecture 9 (\"lexical semantics\") and in the `7.embeddings/TFIDF.ipynb` notebook. Write a method for extracting the 10 terms with highest tf-idf score for each document in a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_words(docs):\n",
    "    \"\"\" Function to return random 10 terms from doc.\n",
    "    \n",
    "    Input: a list of (filename, [spacy tokens]) documents\n",
    "    Returns: a dict mapping \"filename\" -> [list of 10 keyphrases, ranked from highest tf-idf score to lowest]\n",
    " \n",
    "    Used just to illustrate expected output of functions below \"\"\"\n",
    "    \n",
    "    keyphrases={}\n",
    "    \n",
    "    for filename, doc in docs:\n",
    "        tokens=list(set([x.text for x in doc]))\n",
    "        random.shuffle(tokens)\n",
    "  \n",
    "        keyphrases[filename]=tokens[:10]\n",
    "    \n",
    "    return keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms=random_words(original_docs)\n",
    "for filename in [\"Jaws.txt\", \"Harry_Potter_and_the_Philosophers_Stone.txt\", \"Back_to_the_Future.txt\"]:\n",
    "    print(\"\\n%s\\n\" % filename)\n",
    "    print('\\n'.join(terms[os.path.join(inputDir, filename)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_ranking(docs):\n",
    "    \"\"\"\n",
    "    Function to rank terms in document by tf-idf score, and return the top 10 terms\n",
    "    \n",
    "    Input: a list of (filename, [spacy tokens]) documents\n",
    "    Returns: a dict mapping \"filename\" -> [list of 10 keyphrases, ranked from highest tf-idf score to lowest]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def get_tf(tokens):\n",
    "        counter=Counter()\n",
    "        for token in tokens:\n",
    "            counter[token.text]+=1\n",
    "        return counter\n",
    "    \n",
    "    def get_idfs(docs):\n",
    "        counts=Counter()\n",
    "        for _, doc in docs:\n",
    "            doc_types={}\n",
    "            for token in doc:\n",
    "                doc_types[token.text]=1\n",
    "\n",
    "            for word in doc_types:\n",
    "                counts[word]+=1\n",
    "\n",
    "        idfs={}\n",
    "        for term in counts:\n",
    "            idfs[term]=math.log(float(len(docs))/counts[term])\n",
    "\n",
    "        return idfs\n",
    "\n",
    "    idfs=get_idfs(docs)\n",
    "\n",
    "    keyphrases={}\n",
    "    \n",
    "    for filename, doc in docs:\n",
    "        tf=get_tf(doc)\n",
    "        candidates={}\n",
    "        for term in tf:\n",
    "            candidates[term]=tf[term]*idfs[term]\n",
    "\n",
    "        sorted_x = sorted(candidates.items(), key=operator.itemgetter(1), reverse=True)\n",
    "       \n",
    "        keyphrases[filename]=[k for k,v in sorted_x[:10]]\n",
    "    \n",
    "    return keyphrases\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms=tf_idf_ranking(original_docs)\n",
    "for filename in [\"Jaws.txt\", \"Harry_Potter_and_the_Philosophers_Stone.txt\", \"Back_to_the_Future.txt\"]:\n",
    "    print(\"\\n%s\\n\" % filename)\n",
    "    print('\\n'.join(terms[os.path.join(inputDir, filename)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.  Write a method for extracting the 10 terms with highest tf-idf score for each document in a collection that *excludes all proper names*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyphrase_no_proper_nouns(docs):\n",
    "    \"\"\"\n",
    "    Function to rank terms in document by tf-idf score, and return the top 10 terms.  \n",
    "    Constraint: None of the top 10 terms should be proper nouns.\n",
    "    \n",
    "    Input: a list of (filename, [spacy tokens]) documents\n",
    "    Returns: a dict mapping \"filename\" -> [list of 10 keyphrases, ranked from highest tf-idf score to lowest]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def remove_proper_nouns(docs):\n",
    "        new_docs=[]\n",
    "        for filename, doc in docs:\n",
    "            new_doc=[]\n",
    "            for token in doc:\n",
    "                if token.tag_ != \"NNP\" and token.tag_ != \"NNPS\":\n",
    "                    new_doc.append(token)\n",
    "            new_docs.append((filename, new_doc))\n",
    "       \n",
    "        return new_docs\n",
    "            \n",
    "    new_docs=remove_proper_nouns(docs)\n",
    "    terms=tf_idf_ranking(new_docs)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms=keyphrase_no_proper_nouns(original_docs)\n",
    "for filename in [\"Jaws.txt\", \"Harry_Potter_and_the_Philosophers_Stone.txt\", \"Back_to_the_Future.txt\"]:\n",
    "    print(\"\\n%s\\n\" % filename)\n",
    "    print('\\n'.join(terms[os.path.join(inputDir, filename)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.  Write a method for extracting the 10 terms with highest tf-idf score for each document in a collection that *includes only common nouns*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyphrase_only_common_nouns(docs):\n",
    "    \"\"\"\n",
    "    Function to rank terms in document by tf-idf score, and return the top 10 terms.  \n",
    "    Constraint: All of the top 10 terms should be common nouns.\n",
    "    \n",
    "    Input: a list of (filename, [spacy tokens]) documents\n",
    "    Returns: a dict mapping \"filename\" -> [list of 10 keyphrases, ranked from highest tf-idf score to lowest]\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    def remove_proper_nouns(docs):\n",
    "        new_docs=[]\n",
    "        for filename, doc in docs:\n",
    "            new_doc=[]\n",
    "            for token in doc:\n",
    "                if token.tag_ == \"NN\" or token.tag_ == \"NNS\":\n",
    "                    new_doc.append(token)\n",
    "            new_docs.append((filename, new_doc))\n",
    "       \n",
    "        return new_docs\n",
    "            \n",
    "    new_docs=remove_proper_nouns(docs)\n",
    "    terms=tf_idf_ranking(new_docs)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms=keyphrase_only_common_nouns(original_docs)\n",
    "for filename in [\"Jaws.txt\", \"Harry_Potter_and_the_Philosophers_Stone.txt\", \"Back_to_the_Future.txt\"]:\n",
    "    print(\"\\n%s\\n\" % filename)\n",
    "    print('\\n'.join(terms[os.path.join(inputDir, filename)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
