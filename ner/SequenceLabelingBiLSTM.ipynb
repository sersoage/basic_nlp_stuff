{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores sequence labeling using the Twitter NER dataset from the [W-NUT 2016 shared task](https://noisy-text.github.io/2016/ner-shared-task.html#resource)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Input, Embedding, TimeDistributed, Layer, Multiply, Concatenate, Dropout, LSTM, Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename, max_vocab_size):\n",
    "\n",
    "    \"\"\" Load pre-trained word embeddings, reserving 0 for padding symbol and 1 for UNK \"\"\"\n",
    "    \n",
    "    vocab={}\n",
    "    embeddings=[]\n",
    "    with open(filename) as file:\n",
    "        \n",
    "        cols=file.readline().split(\" \")\n",
    "        num_words=int(cols[0])\n",
    "        size=int(cols[1])\n",
    "        embeddings.append(np.zeros(size))  # 0 = 0 padding if needed\n",
    "        embeddings.append(np.zeros(size))  # 1 = UNK\n",
    "        vocab[\"_0_\"]=0\n",
    "        vocab[\"_UNK_\"]=1\n",
    "        \n",
    "        for idx,line in enumerate(file):\n",
    "\n",
    "            if idx+2 >= max_vocab_size:\n",
    "                break\n",
    "\n",
    "            cols=line.rstrip().split(\" \")\n",
    "            val=np.array(cols[1:])\n",
    "            word=cols[0]\n",
    "            \n",
    "            embeddings.append(val)\n",
    "            vocab[word]=idx+2\n",
    "\n",
    "    return np.array(embeddings), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ids(sentences, word_vocab, label_vocab):\n",
    "    \n",
    "    \"\"\" Function to convert a list of sentences (where each sentence is a list of (word, tag) tuples)\n",
    "    into:\n",
    "    -- a list of padded sequences of word ids\n",
    "    -- a list of padded sequence of tag ids\n",
    "    -- a list of sequence lengths (the original token count for each sentence)\n",
    "    \n",
    "    Pads each sequence to the maximum sequence length observed in the sentences input\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    words_ids=[]\n",
    "    sent_lengths=[]\n",
    "    tags_ids=[]\n",
    "    \n",
    "    output_dim=len(label_vocab)+1\n",
    "    \n",
    "    max_length=0\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > max_length:\n",
    "            max_length=len(sentence)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        wids=[]\n",
    "        tids=[]\n",
    "        \n",
    "        for word, tag in sentence:\n",
    "            val = word_vocab[word.lower()] if word.lower() in word_vocab else 1\n",
    "            wids.append(val)\n",
    "            y = to_categorical(label_vocab[tag], num_classes=output_dim)\n",
    "            tids.append(y)\n",
    "        \n",
    "        \n",
    "        for i in range(len(wids),max_length):\n",
    "            wids.append(0)\n",
    "            tids.append(to_categorical(0, num_classes=output_dim))\n",
    "            \n",
    "        words_ids.append(wids)\n",
    "        tags_ids.append(tids)\n",
    "        sent_lengths.append(len(sentence))\n",
    " \n",
    "    return np.array(words_ids), np.array(tags_ids), np.array(sent_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(filename):\n",
    "    \n",
    "    \"\"\" Read input in two-column TSV, one line per word, with sentences delimited by a blank line \"\"\"\n",
    "    \n",
    "    sentences=[]\n",
    "    sentence=[]\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            if len(cols) < 2:\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                sentence=[]\n",
    "                continue\n",
    "                \n",
    "            word=cols[0]\n",
    "            tag=cols[1]\n",
    "            \n",
    "            sentence.append((word, tag))\n",
    "            \n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "            \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_vocab(sentences):\n",
    "    tags={}\n",
    "    # 0 is for masking\n",
    "    tid=1\n",
    "    for sentence in sentences:\n",
    "        for word, tag in sentence:\n",
    "            if tag not in tags:\n",
    "                tags[tag]=tid\n",
    "                tid+=1\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=read_tsv(\"../data/twitter-ner/ner.train.txt\")\n",
    "devData=read_tsv(\"../data/twitter-ner/ner.dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 1, 'B-facility': 2, 'I-facility': 3, 'B-company': 4, 'B-person': 5, 'B-tvshow': 6, 'B-other': 7, 'I-other': 8, 'B-sportsteam': 9, 'I-person': 10, 'B-geo-loc': 11, 'B-movie': 12, 'I-movie': 13, 'I-tvshow': 14, 'B-product': 15, 'I-company': 16, 'B-musicartist': 17, 'I-musicartist': 18, 'I-geo-loc': 19, 'I-product': 20, 'I-sportsteam': 21}\n"
     ]
    }
   ],
   "source": [
    "tag_vocab=get_tag_vocab(data)\n",
    "rev_tags={}\n",
    "for t in tag_vocab:\n",
    "    rev_tags[tag_vocab[t]]=t\n",
    "\n",
    "print(tag_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, word_vocab=load_embeddings(\"../data/glove.twitter.27B.100d.50K.txt.w2v\", 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY, trainS=get_word_ids(data, word_vocab, tag_vocab)\n",
    "devX, devY, devS=get_word_ids(devData, word_vocab, tag_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a bidirectional LSTM for sequence labeling to make predictions about the NER tag for each word in a sentence.  Explore the effect of the lstm size and dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bilstm(embeddings, output_dim, lstm_size=25, dropout_rate=0.25):\n",
    "    \n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "\n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    sentence_lengths = Input(shape=(None,), dtype='int32')\n",
    "\n",
    "    word_embedding_layer = Embedding(vocab_size,\n",
    "                                    word_embedding_dim,\n",
    "                                    weights=[embeddings],\n",
    "                                    trainable=False, mask_zero=True)\n",
    "\n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    bi_lstm = Bidirectional(LSTM(lstm_size, return_sequences=True, activation='relu', dropout=dropout_rate), merge_mode='concat')(embedded_sequences)\n",
    "    preds = TimeDistributed(Dense(output_dim, activation=\"softmax\"))(bi_lstm)\n",
    "\n",
    "    model = Model(inputs=[word_sequence_input, sentence_lengths], outputs=preds)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"acc\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, modelName):\n",
    "    print (model.summary())\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=0, \n",
    "    mode='auto')\n",
    "\n",
    "    checkpoint = ModelCheckpoint(modelName, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "    \n",
    "    model.fit([trainX, trainS], trainY, \n",
    "            validation_data=([devX, devS], devY),\n",
    "            epochs=30, batch_size=32,\n",
    "            callbacks=[checkpoint, early_stopping])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a model on the data and save the one that performs best on the validation data in `bilstm_sequence_labeling.hdf5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mashabelyi/anaconda3/envs/anlp/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/mashabelyi/anaconda3/envs/anlp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 100)         5000200   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 50)          25200     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 22)          1122      \n",
      "=================================================================\n",
      "Total params: 5,026,522\n",
      "Trainable params: 26,322\n",
      "Non-trainable params: 5,000,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /Users/mashabelyi/anaconda3/envs/anlp/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1900 samples, validate on 240 samples\n",
      "Epoch 1/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 1.0093 - acc: 0.8623 - val_loss: 0.5485 - val_acc: 0.9425\n",
      "Epoch 2/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.4337 - acc: 0.9463 - val_loss: 0.3883 - val_acc: 0.9425\n",
      "Epoch 3/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.3549 - acc: 0.9464 - val_loss: 0.3505 - val_acc: 0.9431\n",
      "Epoch 4/30\n",
      "1900/1900 [==============================] - 3s 1ms/step - loss: 0.3226 - acc: 0.9481 - val_loss: 0.3229 - val_acc: 0.9464\n",
      "Epoch 5/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.3014 - acc: 0.9501 - val_loss: 0.3110 - val_acc: 0.9474\n",
      "Epoch 6/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.2872 - acc: 0.9506 - val_loss: 0.3005 - val_acc: 0.9483\n",
      "Epoch 7/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.2745 - acc: 0.9513 - val_loss: 0.2917 - val_acc: 0.9487\n",
      "Epoch 8/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.2641 - acc: 0.9524 - val_loss: 0.2840 - val_acc: 0.9485\n",
      "Epoch 9/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.2498 - acc: 0.9534 - val_loss: 0.2767 - val_acc: 0.9500\n",
      "Epoch 10/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.2411 - acc: 0.9544 - val_loss: 0.2698 - val_acc: 0.9516\n",
      "Epoch 11/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.2328 - acc: 0.9556 - val_loss: 0.2685 - val_acc: 0.9507\n",
      "Epoch 12/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.2276 - acc: 0.9561 - val_loss: 0.2638 - val_acc: 0.9511\n",
      "Epoch 13/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.2192 - acc: 0.9571 - val_loss: 0.2604 - val_acc: 0.9516\n",
      "Epoch 14/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.2159 - acc: 0.9570 - val_loss: 0.2601 - val_acc: 0.9518\n",
      "Epoch 15/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.2064 - acc: 0.9584 - val_loss: 0.2545 - val_acc: 0.9514\n",
      "Epoch 16/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.2031 - acc: 0.9581 - val_loss: 0.2550 - val_acc: 0.9516\n",
      "Epoch 17/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.2001 - acc: 0.9583 - val_loss: 0.2532 - val_acc: 0.9519\n",
      "Epoch 18/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1935 - acc: 0.9588 - val_loss: 0.2476 - val_acc: 0.9517\n",
      "Epoch 19/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1932 - acc: 0.9586 - val_loss: 0.2440 - val_acc: 0.9519\n",
      "Epoch 20/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1890 - acc: 0.9594 - val_loss: 0.2449 - val_acc: 0.9519\n",
      "Epoch 21/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1838 - acc: 0.9604 - val_loss: 0.2502 - val_acc: 0.9519\n",
      "Epoch 22/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1779 - acc: 0.9602 - val_loss: 0.2383 - val_acc: 0.9530\n",
      "Epoch 23/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1762 - acc: 0.9605 - val_loss: 0.2381 - val_acc: 0.9534\n",
      "Epoch 24/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1739 - acc: 0.9610 - val_loss: 0.2423 - val_acc: 0.9524\n",
      "Epoch 25/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1680 - acc: 0.9616 - val_loss: 0.2376 - val_acc: 0.9524\n",
      "Epoch 26/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1625 - acc: 0.9623 - val_loss: 0.2397 - val_acc: 0.9519\n",
      "Epoch 27/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1612 - acc: 0.9624 - val_loss: 0.2384 - val_acc: 0.9538\n",
      "Epoch 28/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1578 - acc: 0.9632 - val_loss: 0.2347 - val_acc: 0.9534\n",
      "Epoch 29/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1537 - acc: 0.9636 - val_loss: 0.2332 - val_acc: 0.9537\n",
      "Epoch 30/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1532 - acc: 0.9639 - val_loss: 0.2338 - val_acc: 0.9532\n"
     ]
    }
   ],
   "source": [
    "model=create_bilstm(embeddings, len(tag_vocab)+1)\n",
    "train(model, \"bilstm_sequence_labeling.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore the performance of the model by predicting the NER tags for a new sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=create_bilstm(embeddings, len(tag_vocab)+1)\n",
    "model.load_weights(\"bilstm_sequence_labeling.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, rev_tags):\n",
    "    text=text.split(\" \")\n",
    "    wids=[]\n",
    "    for t in text:\n",
    "        if t.lower() in word_vocab:\n",
    "            wids.append(word_vocab[t.lower()])\n",
    "        else:\n",
    "            wids.append(0)\n",
    "\n",
    "    wids=np.array(wids)\n",
    "    lengths=np.array([len(wids)])\n",
    "\n",
    "\n",
    "    # lengths=np.expand_dims(lengths, 0)\n",
    "    preds=model.predict([[wids], [lengths]])\n",
    "    y_classes = preds.argmax(axis=-1)\n",
    "\n",
    "\n",
    "    predicted=[rev_tags[t] for t in y_classes[0]]\n",
    "    for w, t in zip(text, predicted):\n",
    "        print(\"%s\\t%s\" % (w,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bill\tB-person\n",
      "Gates\tB-person\n",
      "is\tO\n",
      "the\tO\n",
      "founder\tB-person\n",
      "of\tI-other\n",
      "Microsoft\tB-product\n"
     ]
    }
   ],
   "source": [
    "text=\"Bill Gates is the founder of Microsoft\"\n",
    "predict(text, model, rev_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: You'll notice above that the model gets a token-level validation accuracy around 95 simply due to the high presence of the majority class (\"O\").  That's not a very helpful metric  in this case. Implement F-score for NER.  Remember, the F-score for NER is based on *chunks*; for more, see section 11.3.2 in: of SLP3 [chapter 11](https://web.stanford.edu/~jurafsky/slp3/11.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateF1(gold_sequences, predicted_sequences):\n",
    "    \n",
    "    \"\"\" Function to calculate the precision, recall and F-score over labeled chunks in the gold and predicted\n",
    "    input sequences.  Each input parameter contains a list of label sequences (one label for each word in the\n",
    "    sentence). In the following example, `gold_sequences` and `predicted_sequences` both contain two sentences\n",
    "    (the first has 7 words/tags, and the second has 3 words/tags):\n",
    "    \n",
    "    gold_sequences=[[\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"B-ORG\"], [\"O\", \"O\", \"O\"]]\n",
    "    predicted_sequences=[[\"B-PER\", \"O\", \"O\", \"O\", \"B-PER\", \"O\", \"B-ORG\"], [\"O\", \"O\", \"O\"]]\n",
    "    \n",
    "    Returns tuple of (precision, recall, F-score)\n",
    "    \n",
    "    \"\"\"\n",
    "    def get_entities(sequences):\n",
    "        \n",
    "        ents=[]\n",
    "\n",
    "        for s_idx in range(len(sequences)):\n",
    "            \n",
    "            sent=sequences[s_idx]\n",
    "            \n",
    "            start=None\n",
    "            startCat=None\n",
    "\n",
    "            for w_idx in range(len(sent)):\n",
    "                tag=sent[w_idx]\n",
    "                parts=tag.split(\"-\")\n",
    "                BIO=\"O\"\n",
    "                if len(parts) == 2:\n",
    "                    BIO=parts[0]\n",
    "                    cat=parts[1]\n",
    "\n",
    "                if BIO == \"B\" or BIO == \"O\":\n",
    "                    if start != None:\n",
    "                        end=w_idx-1\n",
    "\n",
    "                        ents.append((s_idx, start, end, startCat))\n",
    "\n",
    "                        start=None\n",
    "                        startCat=None\n",
    "                        end=None\n",
    "\n",
    "                if BIO == \"B\":\n",
    "                    start=w_idx\n",
    "                    startCat=cat\n",
    "\n",
    "            if start != None:\n",
    "                ents.append((s_idx, start, len(sent)-1, startCat))\n",
    "\n",
    "        return ents\n",
    "        \n",
    "                    \n",
    "    gold_ents=get_entities(gold_sequences)\n",
    "    pred_ents=get_entities(predicted_sequences)\n",
    "\n",
    "    g_set=set(gold_ents)\n",
    "    p_set=set(pred_ents)\n",
    "    \n",
    "    precision=0\n",
    "    if len(p_set) > 0:\n",
    "        precision=float(len(g_set.intersection(p_set)))/len(p_set)\n",
    "    recall=0\n",
    "    if len(g_set) > 0:\n",
    "        recall=float(len(g_set.intersection(p_set)))/len(g_set)\n",
    "    \n",
    "    F1=0\n",
    "    if precision + recall > 0:\n",
    "        F1=2*precision*recall/(precision+recall)\n",
    "\n",
    "    return precision, recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 0.333, R: 0.500, F: 0.400\n"
     ]
    }
   ],
   "source": [
    "# Example from class on 4/4\n",
    "\n",
    "precision, recall, F1=calculateF1([[\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"B-ORG\"], [\"O\", \"O\", \"O\"]], [[\"B-PER\", \"O\", \"O\", \"O\", \"B-PER\", \"O\", \"B-ORG\"], [\"O\", \"O\", \"O\"]])\n",
    "print(\"P: %.3f, R: %.3f, F: %.3f\" % (precision, recall, F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras by default calculates metrics like accuracy at the batch level (averaging the metric across batches).  F-score, however, is a metric properly calculated over an entire dataset; we can incorporate that into learning by defining a callback function that prints out the validation F-score at the end of each epoch.  Once you've implemented `calculateF1` above, execute the following cells to see the validation F-score while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F_score(Callback):\n",
    "    \n",
    "    def __init__(self, reverse_tag_vocab):\n",
    "        self.reverse_tag_vocab=reverse_tag_vocab\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        valX=self.validation_data[0]\n",
    "        valS=self.validation_data[1]\n",
    "        valY=self.validation_data[2]\n",
    "        \n",
    "        predictions=self.model.predict([valX, valS])\n",
    "        y_classes = predictions.argmax(axis=-1)\n",
    "        truth = valY.argmax(axis=-1)\n",
    "\n",
    "        preds=[]\n",
    "        golds=[]\n",
    "\n",
    "        s,w=y_classes.shape\n",
    "        for i in range(s):\n",
    "            sent_preds=[]\n",
    "            sent_golds=[]\n",
    "            for j in range(int(valS[i])):\n",
    "                sent_golds.append(self.reverse_tag_vocab[truth[i,j]])\n",
    "                sent_preds.append(self.reverse_tag_vocab[y_classes[i,j]])\n",
    "            preds.append(sent_preds)\n",
    "            golds.append(sent_golds)\n",
    "        \n",
    "        precision, recall, F1=calculateF1(golds, preds)\n",
    "        print(\"P: %.3f, R: %.3f, F: %.3f\" % (precision, recall, F1))\n",
    "    \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, modelName):\n",
    "    print (model.summary())\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=0, \n",
    "    mode='auto')\n",
    "\n",
    "    f_score=F_score(rev_tags)\n",
    "    checkpoint = ModelCheckpoint(modelName, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "    \n",
    "    model.fit([trainX, trainS], trainY, \n",
    "            validation_data=([devX, devS], devY),\n",
    "            epochs=30, batch_size=32,\n",
    "            callbacks=[f_score, checkpoint, early_stopping])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, None, 100)         5000200   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, None, 50)          25200     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, None, 22)          1122      \n",
      "=================================================================\n",
      "Total params: 5,026,522\n",
      "Trainable params: 26,322\n",
      "Non-trainable params: 5,000,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1900 samples, validate on 240 samples\n",
      "Epoch 1/30\n",
      "1900/1900 [==============================] - 5s 2ms/step - loss: 1.2699 - acc: 0.7493 - val_loss: 0.5312 - val_acc: 0.9425\n",
      "P: 0.000, R: 0.000, F: 0.000\n",
      "Epoch 2/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.4310 - acc: 0.9467 - val_loss: 0.4084 - val_acc: 0.9425\n",
      "P: 0.000, R: 0.000, F: 0.000\n",
      "Epoch 3/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.3671 - acc: 0.9466 - val_loss: 0.3660 - val_acc: 0.9425\n",
      "P: 0.000, R: 0.000, F: 0.000\n",
      "Epoch 4/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.3342 - acc: 0.9471 - val_loss: 0.3366 - val_acc: 0.9438\n",
      "P: 0.100, R: 0.007, F: 0.014\n",
      "Epoch 5/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.3051 - acc: 0.9491 - val_loss: 0.3153 - val_acc: 0.9472\n",
      "P: 0.343, R: 0.090, F: 0.142\n",
      "Epoch 6/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.2870 - acc: 0.9504 - val_loss: 0.3005 - val_acc: 0.9467\n",
      "P: 0.342, R: 0.097, F: 0.151\n",
      "Epoch 7/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.2704 - acc: 0.9514 - val_loss: 0.2902 - val_acc: 0.9469\n",
      "P: 0.368, R: 0.104, F: 0.163\n",
      "Epoch 8/30\n",
      "1900/1900 [==============================] - 6s 3ms/step - loss: 0.2588 - acc: 0.9528 - val_loss: 0.2819 - val_acc: 0.9474\n",
      "P: 0.410, R: 0.119, F: 0.185\n",
      "Epoch 9/30\n",
      "1900/1900 [==============================] - 6s 3ms/step - loss: 0.2490 - acc: 0.9531 - val_loss: 0.2774 - val_acc: 0.9481\n",
      "P: 0.487, R: 0.142, F: 0.220\n",
      "Epoch 10/30\n",
      "1900/1900 [==============================] - 6s 3ms/step - loss: 0.2401 - acc: 0.9547 - val_loss: 0.2707 - val_acc: 0.9489\n",
      "P: 0.455, R: 0.149, F: 0.225\n",
      "Epoch 11/30\n",
      "1900/1900 [==============================] - 5s 2ms/step - loss: 0.2331 - acc: 0.9548 - val_loss: 0.2685 - val_acc: 0.9492\n",
      "P: 0.438, R: 0.157, F: 0.231\n",
      "Epoch 12/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.2288 - acc: 0.9545 - val_loss: 0.2642 - val_acc: 0.9499\n",
      "P: 0.400, R: 0.164, F: 0.233\n",
      "Epoch 13/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.2188 - acc: 0.9567 - val_loss: 0.2595 - val_acc: 0.9505\n",
      "P: 0.490, R: 0.179, F: 0.262\n",
      "Epoch 14/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.2144 - acc: 0.9571 - val_loss: 0.2564 - val_acc: 0.9507\n",
      "P: 0.444, R: 0.179, F: 0.255\n",
      "Epoch 15/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.2090 - acc: 0.9572 - val_loss: 0.2555 - val_acc: 0.9508\n",
      "P: 0.443, R: 0.201, F: 0.277\n",
      "Epoch 16/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.2045 - acc: 0.9579 - val_loss: 0.2546 - val_acc: 0.9514\n",
      "P: 0.481, R: 0.194, F: 0.277\n",
      "Epoch 17/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1998 - acc: 0.9580 - val_loss: 0.2535 - val_acc: 0.9516\n",
      "P: 0.444, R: 0.209, F: 0.284\n",
      "Epoch 18/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1935 - acc: 0.9589 - val_loss: 0.2503 - val_acc: 0.9536\n",
      "P: 0.516, R: 0.246, F: 0.333\n",
      "Epoch 19/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1891 - acc: 0.9595 - val_loss: 0.2455 - val_acc: 0.9536\n",
      "P: 0.516, R: 0.239, F: 0.327\n",
      "Epoch 20/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.1848 - acc: 0.9592 - val_loss: 0.2459 - val_acc: 0.9543\n",
      "P: 0.500, R: 0.246, F: 0.330\n",
      "Epoch 21/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.1817 - acc: 0.9595 - val_loss: 0.2457 - val_acc: 0.9534\n",
      "P: 0.531, R: 0.254, F: 0.343\n",
      "Epoch 22/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.1765 - acc: 0.9607 - val_loss: 0.2403 - val_acc: 0.9532\n",
      "P: 0.530, R: 0.261, F: 0.350\n",
      "Epoch 23/30\n",
      "1900/1900 [==============================] - 3s 1ms/step - loss: 0.1712 - acc: 0.9611 - val_loss: 0.2475 - val_acc: 0.9529\n",
      "P: 0.559, R: 0.246, F: 0.342\n",
      "Epoch 24/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.1657 - acc: 0.9615 - val_loss: 0.2468 - val_acc: 0.9531\n",
      "P: 0.557, R: 0.254, F: 0.349\n",
      "Epoch 25/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1626 - acc: 0.9623 - val_loss: 0.2436 - val_acc: 0.9545\n",
      "P: 0.537, R: 0.269, F: 0.358\n",
      "Epoch 26/30\n",
      "1900/1900 [==============================] - 5s 2ms/step - loss: 0.1599 - acc: 0.9629 - val_loss: 0.2471 - val_acc: 0.9536\n",
      "P: 0.529, R: 0.269, F: 0.356\n",
      "Epoch 27/30\n",
      "1900/1900 [==============================] - 5s 2ms/step - loss: 0.1579 - acc: 0.9627 - val_loss: 0.2395 - val_acc: 0.9543\n",
      "P: 0.552, R: 0.276, F: 0.368\n",
      "Epoch 28/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1518 - acc: 0.9635 - val_loss: 0.2426 - val_acc: 0.9529\n",
      "P: 0.485, R: 0.246, F: 0.327\n",
      "Epoch 29/30\n",
      "1900/1900 [==============================] - 4s 2ms/step - loss: 0.1505 - acc: 0.9635 - val_loss: 0.2441 - val_acc: 0.9547\n",
      "P: 0.561, R: 0.276, F: 0.370\n",
      "Epoch 30/30\n",
      "1900/1900 [==============================] - 3s 2ms/step - loss: 0.1475 - acc: 0.9645 - val_loss: 0.2379 - val_acc: 0.9539\n",
      "P: 0.528, R: 0.284, F: 0.369\n"
     ]
    }
   ],
   "source": [
    "model=create_bilstm(embeddings, len(tag_vocab)+1)\n",
    "train(model, \"bilstm_sequence_labeling.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
