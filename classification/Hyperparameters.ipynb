{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores text classification, introducing a majority class baseline and analyzing the affect of hyperparameter choices on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            label=cols[0]\n",
    "            text=cols[1]\n",
    "            # sample text data is already tokenized; if yours is not, do so here            \n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "directory=\"../data/text_classification_sample_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY=read_data(\"%s/train.tsv\" % directory)\n",
    "devX, devY=read_data(\"%s/dev.tsv\" % directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_class(trainY, devY):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baselines are critical as a point of reference to understand how well a text classification method is performing.  One of the simplest of these is the *majority class* baseline: for every point in the test data, predict the label that shows up most frequently **in the training data**.  Implement that basline for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_class(trainY,devY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn's [GridSearchCV](https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html) is a convenient function for evaluating performance across a range of parameters.  For more control, let's write our own grid search function here.  Explore the performance for different parameter settings of [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) (e.g., binary, stopword removal, lowercasing, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "names=[]\n",
    "\n",
    "feat_vals=[50, 100, 500, 1000, 5000, 10000, 50000]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(trainY)\n",
    "Y_train=le.transform(trainY)\n",
    "Y_dev=le.transform(devY)\n",
    "\n",
    "idx=0\n",
    "\n",
    "for feat_val in feat_vals:\n",
    "\n",
    "    # split the string on whitespace because we assume it has already been tokenized\n",
    "    vectorizer = CountVectorizer(max_features=feat_val, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
    "\n",
    "    X_train = vectorizer.fit_transform(trainX)\n",
    "    X_dev = vectorizer.transform(devX)\n",
    "\n",
    "    print (\"%s of %s trials\" % (idx, len(feat_vals)))\n",
    "\n",
    "    logreg = linear_model.LogisticRegression(C=1.0, solver='lbfgs', penalty='l2')\n",
    "    logreg.fit(X_train, Y_train)\n",
    "    scores.append(logreg.score(X_dev, Y_dev))\n",
    "    names.append(\"feat_value:%s\" % (feat_val))\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot these results (may need to execute twice to diplay graph)\n",
    "pd_results=pd.DataFrame({\"value\":names, \"accuracy\":scores})\n",
    "pd_results.plot.bar(x='value', y='accuracy', figsize=(14,6))\n",
    "pd_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters interact with each other (like the number of features and the regularization strength). Perform grid search on a combination of features to evaluate how their interaction affects accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "names=[]\n",
    "\n",
    "feat_vals=[50, 100, 500, 1000, 5000, 10000, 50000]\n",
    "C_values=[0.001, 0.1, 1, 5, 10]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(trainY)\n",
    "Y_train=le.transform(trainY)\n",
    "Y_dev=le.transform(devY)\n",
    "\n",
    "idx=0\n",
    "\n",
    "for feat_val in feat_vals:\n",
    "\n",
    "    # split the string on whitespace because we assume it has already been tokenized\n",
    "    vectorizer = CountVectorizer(max_features=feat_val, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
    "\n",
    "    X_train = vectorizer.fit_transform(trainX)\n",
    "    X_dev = vectorizer.transform(devX)\n",
    "\n",
    "    for C_val in C_values:\n",
    "        \n",
    "        print (\"%s of %s trials\" % (idx, len(feat_vals)*len(C_values)))\n",
    "\n",
    "        logreg = linear_model.LogisticRegression(C=C_val, solver='lbfgs', penalty='l2')\n",
    "        logreg.fit(X_train, Y_train)\n",
    "        scores.append(logreg.score(X_dev, Y_dev))\n",
    "        names.append(\"feat_value:%s-C:%s\" % (feat_val, C_val))\n",
    "        idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_results=pd.DataFrame({\"value\":names, \"accuracy\":scores})\n",
    "pd_results.plot.bar(x='value', y='accuracy', figsize=(14,6))\n",
    "pd_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
