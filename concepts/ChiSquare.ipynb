{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import operator\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets_from_json(filename):\n",
    "    tweets=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        data=json.load(file)\n",
    "        for tweet in data:\n",
    "            tweets.append(tweet[\"text\"])\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets=read_tweets_from_json(\"../data/trump_tweets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoc_tweets=read_tweets_from_json(\"../data/aoc_tweets.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore your assumptions between the words you think will most distinguish the tweets of Donald Trump from those Alexandria Ocasio-Cortez.  Before looking at the data, what words do you think will be comparatively distinct to both?  (If you're not familiar with either, see http://twitter.com/realDonaldTrump and http://twitter.com/AOC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tweets_to_tokens(tweets):\n",
    "    tokens=[]\n",
    "    for tweet in tweets:\n",
    "        tokens.extend(nltk.casual_tokenize(tweet))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(tokens):\n",
    "    counts=Counter()\n",
    "    for token in tokens:\n",
    "        counts[token]+=1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\chi^2$ test as used in the comparison of different texts is designed to measure how statistically significant the distriubtion of counts in a 2x2 contingency table is.  Use the following function to analyze the difference between these accounts.  How do the most distinct terms comport with your assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square(one_counts, two_counts):\n",
    "\n",
    "    one_sum=0.\n",
    "    two_sum=0.\n",
    "    vocab={}\n",
    "    for word in one_counts:\n",
    "        one_sum+=one_counts[word]\n",
    "        vocab[word]=1\n",
    "    for word in two_counts:\n",
    "        vocab[word]=1\n",
    "        two_sum+=two_counts[word]\n",
    "\n",
    "    N=one_sum+two_sum\n",
    "    vals={}\n",
    "    \n",
    "    for word in vocab:\n",
    "        O11=one_counts[word]\n",
    "        O12=two_counts[word]\n",
    "        O21=one_sum-one_counts[word]\n",
    "        O22=two_sum-two_counts[word]\n",
    "        \n",
    "        # We'll use the simpler form given in Manning and Schuetze (1999) \n",
    "        # for 2x2 contingency tables: \n",
    "        # https://nlp.stanford.edu/fsnlp/promo/colloc.pdf, equation 5.7\n",
    "        \n",
    "        vals[word]=(N*(O11*O22 - O12*O21)**2)/((O11 + O12)*(O11+O21)*(O12+O22)*(O21+O22))\n",
    "        \n",
    "    sorted_chi = sorted(vals.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    one=[]\n",
    "    two=[]\n",
    "    for k,v in sorted_chi:\n",
    "        if one_counts[k]/one_sum > two_counts[k]/two_sum:\n",
    "            one.append(k)\n",
    "        else:\n",
    "            two.append(k)\n",
    "    \n",
    "    print (\"@realdonaldtrump:\\n\")\n",
    "    for k in one[:20]:\n",
    "        print(\"%s\\t%s\" % (k,vals[k]))\n",
    "\n",
    "    print (\"\\n\\n@AOC:\\n\")\n",
    "    for k in two[:20]:\n",
    "        print(\"%s\\t%s\" % (k,vals[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tokens=convert_tweets_to_tokens(trump_tweets)\n",
    "trump_counts=get_counts(trump_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoc_tokens=convert_tweets_to_tokens(aoc_tweets)\n",
    "aoc_counts=get_counts(aoc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_square(trump_counts, aoc_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw earlier that $\\chi^2$ is not a perfect estimator since it doesn't account for the burstiness of language (the tendency of mentions of the same word to clump together in a text).  Do you expect this to still hold on Twitter?  Why or why not?  How are the differences identified by a $\\chi^2$ similar to those by Mann-Whitney?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
