{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores methods for comparing two different textual datasets to identify the terms that are distinct to each one:\n",
    "\n",
    "* Difference of proportions (described in [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf) section 3.2.2\n",
    "* Mann-Whitney rank-sums test (described in [Kilgarriff 2001, Comparing Corpora](https://www.sketchengine.eu/wp-content/uploads/comparing_corpora_2001.pdf), section 2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, operator\n",
    "from collections import Counter\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the convote data is already tokenized so just split on whitespace\n",
    "repub_tokens=open(\"../data/repub.convote.txt\", encoding=\"utf-8\").read().split(\" \")\n",
    "dem_tokens=open(\"../data/dem.convote.txt\", encoding=\"utf-8\").read().split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: First, calculate the simple \"difference of proportions\" measure from Monroe et al.'s \"Fighting Words\", section 3.2.2.  What are the top ten terms in this measurement that are most republican and most democrat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_differences(one_tokens, two_tokens):\n",
    "    one_N=len(one_tokens)\n",
    "    two_N=len(two_tokens)\n",
    "    \n",
    "    one_counts=Counter()\n",
    "    two_counts=Counter()\n",
    "    \n",
    "    vocab={}\n",
    "    for token in one_tokens:\n",
    "        one_counts[token]+=1\n",
    "        vocab[token]=1\n",
    "        \n",
    "    for token in two_tokens:\n",
    "        two_counts[token]+=1    \n",
    "        vocab[token]=1\n",
    "        \n",
    "    differences={}\n",
    "    for word in vocab:\n",
    "        freq1=one_counts[word]/one_N\n",
    "        freq2=two_counts[word]/two_N\n",
    "        \n",
    "        diff=freq1-freq2\n",
    "        differences[word]=diff\n",
    "        \n",
    "    return differences\n",
    "\n",
    "def difference_of_proportions(one_tokens, two_tokens):\n",
    "\n",
    "    differences=count_differences(one_tokens, two_tokens)\n",
    "    \n",
    "    sorted_differences = sorted(differences.items(), key=operator.itemgetter(1))\n",
    "    print (\"More Republican:\")\n",
    "    for k,v in sorted_differences[:10]:\n",
    "        print (\"%s\\t%s\" % (k,v))\n",
    "    print(\"\\nMore Democrat:\")\n",
    "    for k,v in reversed(sorted_differences[-10:]):\n",
    "\n",
    "        print (\"%s\\t%s\" % (k,v))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More Republican:\n",
      "i\t-0.002870948015418236\n",
      "we\t-0.0020739540633471117\n",
      "and\t-0.0017279456625680124\n",
      "of\t-0.0014950519581076668\n",
      ",\t-0.00105321588184943\n",
      "chairman\t-0.0009598934247981516\n",
      "that\t-0.000945583476245123\n",
      "as\t-0.0009124972356492223\n",
      "gentleman\t-0.0008093810284795912\n",
      "a\t-0.0008020309007514565\n",
      "\n",
      "More Democrat:\n",
      "not\t0.0015745433184340962\n",
      "$\t0.0015095648428079297\n",
      "cuts\t0.001031315818425968\n",
      "bill\t0.0010228370409021796\n",
      "republican\t0.001001288082839861\n",
      "budget\t0.0009261863664701928\n",
      "billion\t0.0008820967153998979\n",
      "would\t0.0007701123575280444\n",
      "health\t0.0007538336601492987\n",
      "for\t0.0007352277281844066\n"
     ]
    }
   ],
   "source": [
    "difference_of_proportions(dem_tokens, repub_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply analyzing the difference in relative frequencies has a number of downsides: 1.) As Monroe et al (2009) points out (and we can see here as well), it tends to emphasize high-frequency words (be sure you understand why).  2.) We're not measuring whether a difference is statistically meaningful or just due to chance; the $\\chi^2$ test is one method (described in Kilgarriff 2001 and in the context of collocations in Manning and Schuetze [here](https://nlp.stanford.edu/fsnlp/promo/colloc.pdf)) that addresses the desideratum of finding statistically significant terms, but it too has another downside: 3.) Simply counting up the total number of mentions of a term doesn't account for the \"burstiness\" of language -- if we see the word \"Dracula\" in a text, we're probably going to see it again in that same text.  The occurrence of words are not independent random events; they are tightly coupled with each other. If we're trying to understanding the robust differences between two corpora, we might prefer to prioritize words that show up more frequently *everywhere* in corpus A (but not in corpus B) over those that show up only very frequently within narrow slice of A (such as one text in a genre, one chapter in a book, or one speaker when measuring the differences between policital parties)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2 (check-plus): One measure that does account for this burstiness is the adaptation by corpus linguistics of the non-parametric Mann-Whitney rank-sum test. The specific adaptation of this test for text is described in Kilgarriff 2001, section 2.3.  Implement this test using a fixed chunk size of 500 and the [scikit-learn mannwhitneyu function](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html); what are the top ten terms in this measurement that are most republican and most democrat? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a sequence of tokens into counts for each chunkLength-word window\n",
    "def get_chunk_counts(tokens, chunkLength):\n",
    "    chunks=[]\n",
    "    for i in range(0, len(tokens), chunkLength):\n",
    "            counts=Counter()\n",
    "            for j in range(chunkLength):\n",
    "                if i+j < len(tokens):\n",
    "                    counts[tokens[i+j]]+=1\n",
    "            chunks.append(counts)\n",
    "    return chunks\n",
    "\n",
    "# calculate mann-whitney test for each word in vocabulary\n",
    "def mann_whitney(one_tokens, two_tokens):\n",
    "\n",
    "    chunkLength=500\n",
    "    one_chunks=get_chunk_counts(one_tokens, chunkLength)\n",
    "    two_chunks=get_chunk_counts(two_tokens, chunkLength)\n",
    "    \n",
    "    # vocab is the union of terms in both sets\n",
    "    vocab={}\n",
    "    \n",
    "    for chunk in one_chunks:\n",
    "        for word in chunk:\n",
    "            vocab[word]=1\n",
    "    for chunk in two_chunks:\n",
    "        for word in chunk:\n",
    "            vocab[word]=1\n",
    "    \n",
    "    pvals={}\n",
    "    \n",
    "    for word in vocab:\n",
    "        \n",
    "        a=[]\n",
    "        b=[]\n",
    "        \n",
    "        # Note a and b can be different lengths (i.e., different sample sizes)\n",
    "        # \n",
    "        # See Mann and Whitney (1947), \"On a Test of Whether one of Two Random \n",
    "        # Variables is Stochastically Larger than the Other\"\n",
    "        # https://projecteuclid.org/download/pdf_1/euclid.aoms/1177730491\n",
    "        \n",
    "        # (This is part of their innovation over the case of equal sample sizes in Wilcoxon 1945)\n",
    "        \n",
    "        for chunk in one_chunks:\n",
    "            a.append(chunk[word])\n",
    "        for chunk in two_chunks:\n",
    "            b.append(chunk[word])\n",
    "\n",
    "        statistic,pval=mannwhitneyu(a,b, alternative=\"two-sided\")\n",
    "        \n",
    "        # We'll use the p-value as our quantity of interest.  [Note in the normal appproximation\n",
    "        # that Mann-Whitney uses to assess significance for large sample sizes, the significance \n",
    "        # of the raw statistic depends on the number of ties in the data, so the statistic itself\n",
    "        # isn't exactly comparable across different words]\n",
    "        pvals[word]=pval\n",
    "\n",
    "    return pvals\n",
    "    \n",
    "# calculate mann-whitneyfor each word in vocabulary and present the top 10 terms for each group\n",
    "def mann_whitney_analysis(one_tokens, two_tokens):\n",
    "    \n",
    "    pvals=mann_whitney(one_tokens, two_tokens)\n",
    "    \n",
    "    # Mann-Whitney tells us the significance of a term's difference in two groups, but we also \n",
    "    # need the directionality of that difference (whether it's used more by group A or group B. \n",
    "    \n",
    "    # Let's use our difference-in-proportions function above to check the directionality.  \n",
    "    # [Note we could also measure directionality by checking whether the Mann-Whitney statistic\n",
    "    # is greater or less than the mean=len(one_chunks)*len(two_chunks)*0.5.]\n",
    "\n",
    "    differences=count_differences(one_tokens, two_tokens)\n",
    "    \n",
    "    one_terms={k : pvals[k] for k in pvals if differences[k] <= 0}\n",
    "    two_terms={k : pvals[k] for k in pvals if differences[k] > 0}\n",
    "    \n",
    "    sorted_pvals = sorted(one_terms.items(), key=operator.itemgetter(1))\n",
    "    print(\"More Republican:\\n\")\n",
    "    for k,v in sorted_pvals[:10]:\n",
    "        print(\"%s\\t%.15f\" % (k,v))\n",
    "\n",
    "    print(\"\\nMore Democrat:\\n\")\n",
    "    sorted_pvals = sorted(two_terms.items(), key=operator.itemgetter(1))\n",
    "    for k,v in sorted_pvals[:10]:\n",
    "        print(\"%s\\t%.15f\" % (k,v))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More Republican:\n",
      "\n",
      "growth\t0.000000000001727\n",
      "i\t0.000000000003221\n",
      "important\t0.000000006857649\n",
      "economy\t0.000000011767342\n",
      "sensenbrenner\t0.000000016397606\n",
      "may\t0.000000031020346\n",
      "gentleman\t0.000000037460807\n",
      "thank\t0.000000058448892\n",
      "consume\t0.000000296135922\n",
      "forward\t0.000000309308704\n",
      "\n",
      "More Democrat:\n",
      "\n",
      "republican\t0.000000000000000\n",
      "cuts\t0.000000000000000\n",
      "republicans\t0.000000000000000\n",
      "majority\t0.000000000000000\n",
      "cut\t0.000000000000001\n",
      "billion\t0.000000000000008\n",
      "--\t0.000000000000044\n",
      "opposition\t0.000000000000079\n",
      "$\t0.000000000000388\n",
      "fails\t0.000000000000422\n"
     ]
    }
   ],
   "source": [
    "mann_whitney_analysis(dem_tokens, repub_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
