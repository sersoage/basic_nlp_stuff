{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores linear regression with L2 (ridge) and L1 (lasso) regularization, using the movie box office prediction data from [Joshi et al. 2010](http://www.cs.cmu.edu/~ark/movie$-data/).  Be sure to install beautifulsoup (a great python library for reading XML).\n",
    "\n",
    "```sh \n",
    "conda install beautifulsoup4=4.7.1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import sklearn.metrics\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_movie_data(filename):\n",
    "    \n",
    "    trainX=[]\n",
    "    Y_train=[]\n",
    "    \n",
    "    testX=[]\n",
    "    Y_test=[]\n",
    "    \n",
    "    with open(filename) as file:\n",
    "        soup=BeautifulSoup(file)\n",
    "        movies=soup.findAll('instance')\n",
    "        for movie in movies:\n",
    "            split=movie[\"subpop\"]\n",
    "            y=float(movie.find('regy')[\"yvalue\"])\n",
    "\n",
    "            # we'll just take the first review in the data (each movie has multiple reviews)\n",
    "            review=movie.find('text')\n",
    "            \n",
    "            tokens=nltk.word_tokenize(review.text)\n",
    "            words=' '.join(tokens)\n",
    "            if split == \"train\":\n",
    "                trainX.append(words)\n",
    "                Y_train.append(y)\n",
    "            elif split == \"test\":\n",
    "                testX.append(words)\n",
    "                Y_test.append(y)\n",
    "   \n",
    "    return trainX, Y_train, testX, Y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_weights(learned_model, vocab, num_to_print, printZero=True):\n",
    "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    sort_index = np.argsort(learned_model.coef_)\n",
    "    \n",
    "    for k in reversed(sort_index[-num_to_print:]):\n",
    "        if learned_model.coef_[k] != 0 or printZero:\n",
    "            print (\"%.5f\\t%s\" % (learned_model.coef_[k], reverse_vocab[k] ))\n",
    "        \n",
    "    print()\n",
    "\n",
    "    for k in sort_index[:num_to_print]:\n",
    "        if learned_model.coef_[k] != 0 or printZero:\n",
    "            print (\"%.5f\\t%s\" % (learned_model.coef_[k], reverse_vocab[k] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, Y_train, testX, Y_test=read_movie_data(\"../data/7domains-train-dev.tl.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000, ngram_range=(1,2), lowercase=True, strip_accents=None, binary=True)\n",
    "X_train = vectorizer.fit_transform(trainX)\n",
    "X_test = vectorizer.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression is linear regression with L2 regularization; how does varying the regularization strength affect the accuracy (MAE)?  How does it affect the rank order of the most informative coefficients?  Play around with the parameters of the CountVectorizer above (varying the number of max_features, increasing the ngram range to include bigrams, trigrams, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# higher values of alpha = stronger regularization\n",
    "ridge_regression = linear_model.Ridge(alpha=100, fit_intercept=True)\n",
    "ridge_regression.fit(X_train, (Y_train))\n",
    "preds=ridge_regression.predict(X_test)\n",
    "mae=sklearn.metrics.mean_absolute_error(preds, (Y_test))\n",
    "print(\"MAE: %.3f\" % mae)\n",
    "analyze_weights(ridge_regression, vectorizer.vocabulary_, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso is linear regression with L1 regularization, which pressures coefficients to not only be close to zero, but exactly zero.  Lasso provides features selection as a result of this, since parameters with 0 value are effectively removed from the model. How does varying the regularization strength here affect the number of non-zero coefficients?  How does it affect the rank order of the most informative coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso(alpha=100, fit_intercept=True, max_iter=10000)\n",
    "lasso.fit(X_train, (Y_train))\n",
    "preds=lasso.predict(X_test)\n",
    "mae=sklearn.metrics.mean_absolute_error(preds, (Y_test))\n",
    "print(\"MAE: %.3f\" % mae)\n",
    "\n",
    "count=0\n",
    "for val in lasso.coef_:\n",
    "    count+=1 if val != 0 else 0\n",
    "\n",
    "print(\"Nonzero features: %s\\n\" % count)\n",
    "analyze_weights(lasso, vectorizer.vocabulary_, 5, printZero=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
